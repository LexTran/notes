# 深度学习



# 模型压缩

## 量化$^{[1]}$

模型量化将浮点运算量化为低比特定点运算，从而降低计算复杂度、参数大小和内存损耗，但是量化过程中精度会大量损失，量化为极低比特如4比特、2比特时更加严重，此外梯度进行量化也会面临挑战

### 压缩参数

模型量化最初提出时就是为了压缩参数，韩松在ICLR2016上获得Best Paper的论文里首次提出了模型量化方法，使用k-means聚类让相邻的数值聚类到同一个中心从而复用同一个数值。

### 提升速度

我们量化时希望能够带来速度的提升，但很多量化都无法提速，原因如下：
首先引入一个概念——理论计算峰值：单位时钟周期内能完成的计算个数 $\times$ 芯片频率

什么样的量化能够带来速度提升呢？根据研究来看需要满足两个条件：
1. 量化数值的计算在部署硬件上的计算峰值更高
2. 量化算法引入的额外开销小

现存提速概率较大的量化算法主要有以下三类：
1. 二值化
2. 线性量化：还可以进一步细分为非对称、对称以及ristretto
3. 对数量化

### 降低内存

量化还有一个目标是降低模型运行时内存，但是大部分量化方法专注于对卷积过程进行量化来减小参数规模，但是运行时内存其实受到激活函数影响更大

只有将量化拓展到网络的各层的激活值才能带来显著的降低内存的效果，但是这样做也会带来更大的精度损失


## 剪枝$^{[2]}$

所谓模型剪枝(prune)就是在尽量少减小精度的前提下取出网络中冗余的channel，filter，neuron或者layer从而得到更轻量网络的方法。

可以分为四个粒度：
1. 细粒度剪枝(fine-grained)：对连接或者神经元剪枝
2. 向量剪枝(vector-level)：属于对卷积核内部的剪枝
3. 核剪枝(kernel-level)：去除卷积核，同时丢弃输入通道中对应通道
4. 滤波器剪枝(filter-level)：对整个卷积核组进行剪枝，会导致输出特征通道数改变
> 滤波器剪枝导致输出特征通道数改变，所以不需要专门的算法支持，称为结构化剪枝；另外三种粒度下，网络拓扑结构发生变化，需要专门的算法支持稀疏运算，称为非结构化剪枝

剪枝步骤：
1. 训练网络
2. 评估权重和神经元的重要性，排序后删除不重要的权重和神经元
3. 移除权重后会导致精度一定程度下降，所以要进行微调，也就是用训练数据更新一下参数
4. 为了不使精度下降过于严重，不要一次丢掉太多权重和神经元，迭代算法

## 知识蒸馏$^{[3]}$



## Neural Architecture Search(NAS)$^{[4]}$


# 参考资料

<a href="https://zhuanlan.zhihu.com/p/132561405">[1] 商汤科技. 模型量化了解一下？[OL] https://zhuanlan.zhihu.com/p/132561405</a>
<a href="https://www.jianshu.com/p/8e1209d3127a">[2] 加油11dd23. 模型剪枝[OL] https://www.jianshu.com/p/8e1209d3127a</a>
<a href="https://www.jianshu.com/p/5c38872cdc0f">[3] 王同学死磕技术. 知识蒸馏（Knowledge Distilling），让你的模型轻装上阵——keras 实战[OL] https://www.jianshu.com/p/5c38872cdc0f</a>
<a href="https://zhuanlan.zhihu.com/p/166220680">[4] deephub. 神经网络架构搜索(NAS)基础[OL] https://zhuanlan.zhihu.com/p/166220680</a>