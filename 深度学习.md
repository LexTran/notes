---
banner: "https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg2020.cnblogs.com%2Fblog%2F1265283%2F202008%2F1265283-20200810173733956-1928677042.png&refer=http%3A%2F%2Fimg2020.cnblogs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1650721652&t=f479665fc6c5f9abc57d571d2bc8b0cd"
---
---
banner: "https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg2020.cnblogs.com%2Fblog%2F1265283%2F202008%2F1265283-20200810173733956-1928677042.png&refer=http%3A%2F%2Fimg2020.cnblogs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1650721652&t=f479665fc6c5f9abc57d571d2bc8b0cd"
---

# 神经网络与深度学习

## 机器学习基础

### 绪论

贡献度分配问题：深度学习中的模型比较复杂，样本输入到输出之间的数据流会经过多个组件，我们不知道每个组件对最终结果的贡献
一种比较好的解决方案是人工神经网络(Artificial Neural Network, ANN)

目前人工智能主要领域大体分为以下几个：
- 感知：语音信息处理、CV
- 学习：监督学习、无监督学习、强化学习
- 认知：NLP、知识表示、推理、规划、决策

### 机器学习概述

![[AI发展史.png]]
<center style="color:silver">图1.1 AI发展史</center>

==流派==
**符号主义**：也称逻辑主义、心理学派或计算机学派
通过分析人类智能的功能，然后通过计算机来实现这些功能
两个基本假设：
(a) 信息可以用符号来表示
(b) 符号可以通过显式的规则（比如逻辑运算）来操作。人类的认知过程可以看作是符号操作过程。 在人工智能的推理期和知识期，符号主义的方法比较盛行，并取得了大量的成果

**连接主义**：也称作仿生学派或生理学派
是认知科学领域中的一类信息处理的方法和理论。人类的认知过程可以看作是一种信息处理过程。连接主义认为人类的认知过程是由大量简单神经元构成的神经网络中的信息处理过程，而不是符号运算
连接主义模型的主要结构是由大量简单的信息处理单元组成的互联网络，具有非线性、分布式、并行化、局部性计算以及自适应性等特性

#### 机器学习

传统的机器学习关注于学习一个预测模型，首先将数据表示为一组特征，将特征输入预测模型得到预测结果，这种机器学习可以看做浅层学习，它不涉及特征学习，特征主要依靠人工标注提取。

机器学习一般步骤是：
1. 数据预处理：去噪等
2. 特征提取：从原始数据提取特征，如：提取边缘、尺度不变特征变换(Scale Invariant Feature Transform, SIFT)等
3. 特征转换：对特征进行加工，如升降维，降维可用特征抽取和特征选择两种方法。常用特征转换方法有主成分分析(Principal Components Analysis, PCA)、线性判别分析(Linear Discriminant Analysis, LDA)等
4. 预测
实际操作中，大部分机器学习模型性能差不多，传统机器学习问题最终演变成了特征工程问题

#### 表示学习

为了提高机器学习的准确率，我们需要将输入数据转换成有效的特征，更一般性称为表示(Representation)，找到算法自动学习出有效的特征，这就是表示学习。

表示学习的关键问题是解决语义鸿沟(Semantic Gap)问题
> 语义鸿沟：指输入数据的底层特征和高层语义信息之间的不一致性和差异性

两个核心问题：什么是好的“表示”？如何学到好的“表示”？

“好的表示”是一个相对主观的概念，但一般而言，一个好的表示应该：
- 有较强的表示能力，同样大小的向量可以表示更多信息
- 使后续学习任务变简单，需要包含高层语义信息
- 具有一般性
机器学习中，我们常用两种方式表示特征：局部表示和分布式表示

局部表示常以*one-hot*向量形式表示，优点在于：1）离散表示具有很好的可解释性，利于人工归纳和总结特征，并通过特征组合进行高效的特征工程；2）通过多种特征组合得到的表示向量通常是稀疏的二值向量，用于线性模型计算效率很高。但是它的缺点在于：1）维度很高，不能拓展，以颜色为例，每多一种颜色就需要加一个维度；2）无法知道不同特征取值之间的相似度
> *one-hot*向量：有且仅有1位元素为1，其他元素为0的向量

分布式表示通常可以表示为低维度稠密向量，它的表示能力要强很多，而且不同特征取值之间的相似度也容易计算

可以使用神经网络将高维度的局部表示空间$\mathbb{R}^{|V|}$($V$为所有特征构成的向量)映射到低维分布式表示空间$\mathbb R^{D},D<<|V|$
在这个低维空间中，每个特征不再是坐标轴上的点，分散在整个特征空间中。机器学习中把这个过程称作嵌入
> 嵌入：指的是将一个度量空间中的对象映射到另一个度量空间，并尽可能保持不同对象之间的拓扑关系



### 线性模型



## 基础模型

### 前馈神经网络



### 卷积神经网络



### 循环神经网络



### 网络优化与正则化



### 注意力机制与外部记忆



### 无监督学习



### 模型独立的学习方式



## 进阶模型

### 概率图模型



### 深度信念网络



### 深度生成模型



### 深度强化学习



### 序列生成模型




# 模型压缩

## 量化$^{[1]}$

模型量化将浮点运算量化为低比特定点运算，从而降低计算复杂度、参数大小和内存损耗，但是量化过程中精度会大量损失，量化为极低比特如4比特、2比特时更加严重，此外梯度进行量化也会面临挑战

### 压缩参数

模型量化最初提出时就是为了压缩参数，韩松在ICLR2016上获得Best Paper的论文里首次提出了模型量化方法，使用k-means聚类让相邻的数值聚类到同一个中心从而复用同一个数值。

### 提升速度

我们量化时希望能够带来速度的提升，但很多量化都无法提速，原因如下：
首先引入一个概念——理论计算峰值：单位时钟周期内能完成的计算个数 $\times$ 芯片频率

什么样的量化能够带来速度提升呢？根据研究来看需要满足两个条件：
1. 量化数值的计算在部署硬件上的计算峰值更高
2. 量化算法引入的额外开销小

现存提速概率较大的量化算法主要有以下三类：
1. 二值化
2. 线性量化：还可以进一步细分为非对称、对称以及ristretto
3. 对数量化

### 降低内存

量化还有一个目标是降低模型运行时内存，但是大部分量化方法专注于对卷积过程进行量化来减小参数规模，但是运行时内存其实受到激活函数影响更大

只有将量化拓展到网络的各层的激活值才能带来显著的降低内存的效果，但是这样做也会带来更大的精度损失


## 剪枝$^{[2]}$

所谓模型剪枝(prune)就是在尽量少减小精度的前提下取出网络中冗余的channel，filter，neuron或者layer从而得到更轻量网络的方法。

可以分为四个粒度：
1. 细粒度剪枝(fine-grained)：对连接或者神经元剪枝
2. 向量剪枝(vector-level)：属于对卷积核内部的剪枝
3. 核剪枝(kernel-level)：去除卷积核，同时丢弃输入通道中对应通道
4. 滤波器剪枝(filter-level)：对整个卷积核组进行剪枝，会导致输出特征通道数改变
> 滤波器剪枝导致输出特征通道数改变，所以不需要专门的算法支持，称为结构化剪枝；另外三种粒度下，网络拓扑结构发生变化，需要专门的算法支持稀疏运算，称为非结构化剪枝

剪枝步骤：
1. 训练网络
2. 评估权重和神经元的重要性，排序后删除不重要的权重和神经元
3. 移除权重后会导致精度一定程度下降，所以要进行微调，也就是用训练数据更新一下参数
4. 为了不使精度下降过于严重，不要一次丢掉太多权重和神经元，迭代算法

## 知识蒸馏$^{[3]}$



## Neural Architecture Search(NAS)$^{[4]}$


# 参考资料

<a href="https://zhuanlan.zhihu.com/p/132561405">[1] 商汤科技. 模型量化了解一下？[OL] https://zhuanlan.zhihu.com/p/132561405</a>
<a href="https://www.jianshu.com/p/8e1209d3127a">[2] 加油11dd23. 模型剪枝[OL] https://www.jianshu.com/p/8e1209d3127a</a>
<a href="https://www.jianshu.com/p/5c38872cdc0f">[3] 王同学死磕技术. 知识蒸馏（Knowledge Distilling），让你的模型轻装上阵——keras 实战[OL] https://www.jianshu.com/p/5c38872cdc0f</a>
<a href="https://zhuanlan.zhihu.com/p/166220680">[4] deephub. 神经网络架构搜索(NAS)基础[OL] https://zhuanlan.zhihu.com/p/166220680</a>
<a href="https://zhuanlan.zhihu.com/p/143203830">[5] 聿程. 深度学习网络结构设计[OL] https://zhuanlan.zhihu.com/p/143203830</a>
<a href="https://zhuanlan.zhihu.com/p/143256884">[6] 聿程.模型训练中的Tricks[OL] https://zhuanlan.zhihu.com/p/143256884</a>
