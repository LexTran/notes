# 复杂度分析

## 大O复杂度表示法

算法的执行效率就是算法代码执行的时间，但是如何在不运行代码的情况下，用"肉眼"得到代码执行时间呢？
从CPU的角度来看，代码的每一行都执行类似的操作：**读数据-运算-写数据**，尽管CPU执行的指令个数、时间不同，但是我们可以假设每条代码执行时间相同进行估算，设之为$unit\_time$，所有代码的总执行时间$T(n)$与每行代码的执行次数$f(n)$成正比，可得$T(n)=O(f(n))$，这就是大$O$时间复杂度表示法，表示的是代码执行时间随着数据规模增长的变化趋势，也叫做渐进时间复杂度
当$n$足够大时，公式中的低阶、常量和系数对增长趋势影响不大，可以忽略，换句话说我们只需要保留最大量级，即$O(2n^2+3n+2)\approx O(n^2)$

## 时间复杂度分析

具体来说，如何分析时间复杂度呢？可以遵循以下三个方法
1. 只关注循环执行次数最多的一段代码
如上所说，可以忽略掉低阶、常量和系数部分，所以分析算法时间复杂度时，也只需要关注执行次数最多的一段代码
2. 加法法则：总复杂度等于量级最大的那段代码的复杂度
复杂算法中可能涉及到多个子算法的复杂度分析，我们仍然只需要考虑复杂度最高的那一段代码
3. 乘法法则：嵌套代码复杂度等于嵌套内外代码复杂度的乘积
算法中涉及到算法嵌套，A算法中调用了B算法，A算法中B算法在循环中调用了n次，加入把B算法看作普通代码，复杂度为$O(n)$，而B算法本身复杂度为$O(n^2)$，那么算法整体复杂度就是$O(n*n^2)=O(n^3)$

## 常见的复杂度量级

常量阶：$O(1)$
指数阶：$O(2^n)$
对数阶：$O(\log n)$
阶乘阶：$O(n!)$
线性阶：$O(n)$
线性对数阶：$O(n\log n)$
次方阶：$O(n^k)$
以上复杂度可以简单分为多项式量级和非多项式量级，非多项式量级只有两个：$O(n!)$和$O(2^n)$
时间复杂度为非多项式量级的算法问题称作NP问题，NP时间复杂度是非常低效的算法

### O(log n)、O(nlog n)

对数阶的时间复杂度非常常见，但也是最难分析的一种，举个例子
```C
int i=1;
while(i<=n){
	i=i*2;
}
```
从代码中可以看出，每循环一次，i的值就要翻一倍，实际上是一个等比数列，循环的次数就可以利用对数求解，即$x=\log_2 n$
同理，若将循环中翻倍改成翻三倍，循环次数就变为$x=\log_3 n$
那么线性对数阶的复杂度怎么来的呢？利用前面所说的乘法法则，若在一个普通的循环中调用了复杂度为$O(\log n)$的算法，其复杂度就变为了$O(n\log n)$

## 空间复杂度分析

空间复杂度的全称是渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系

### 常见空间复杂度

常量阶：$O(1)$
线性阶：$O(n)$
平方阶：$O(n^2)$

## 最好、最坏、平均情况时间复杂度

我们先来看一个例子：
```c
int find(int[] array, int n, int x){
	int i=0;
	int pos=-1;
	for (; i<n; i++){
		if(array[i]==x){
			pos=i;
			break;
		}
	}
	return pos;
}
```
上述代码描述了在一个无序数组中查找变量出现位置的算法，看起来代码循环了n次，时间复杂度应该是$O(n)$，但事实上我们并不知道变量出现的位置，加入变量出现在第一个位置，复杂度就是$O(1)$，而若变量出现在最后一个位置，复杂度就是$O(n)$

为了表示不同情况下的时间复杂度，引入了最好情况、最坏情况、平均情况时间复杂度三个概念
==最好情况时间复杂度==：最理想的情况下，时间复杂度
==最坏情况时间复杂度==：最糟糕的情况下，时间复杂的
==平均情况时间复杂度==：以上述代码为例，变量x在数列中的位置共有n+1中情况，分别对应$0\sim n-1$的n个位置以及不在数列中，变量x在数列中和不在数列中分别占据$\frac{1}{2}$的概率，而变量x在n个位置上的概率分别为$\frac{1}{n}$，根据概率乘法，x出现在$0\sim n-1$中任意位置的概率就是$\frac{1}{2n}$，总体的时间复杂度也就变成了$1\times \frac{1}{2n}+2\times \frac{1}{2n}+...+n\times \frac{1}{2n}+n\times \frac{1}{2}=\frac{3n+1}{4}$

## 均摊时间复杂度

均摊时间复杂度应用场景非常特殊，很少见，理解即可，下面给出一个例子
```c
int[] array = new int[n];
int count = 0;
void insert(int val){
	if(count == array.length){
		int sum = 0;
		for(int i=0;i<array.length;++i){
			sum = sum+array[i];
		}
		array[0] = sum;
		count = 1;
	}
	array[count] = val;
	++count;
}
```
这段代码实现了一个往数组中插入数据的功能，数组满了以后，使用for循环遍历数组求和并清空数组然后将结果放到数组的第一个位置
针对这个算法，最好情况和评价情况的时间复杂度都是$O(1)$，只有最坏情况下时间复杂度是$O(n)$，对比这一例子和上一小节的例子，会发现它们有很大不同
1. 上一节的例子中极端情况下复杂度才为$O(1)$，但本节的例子在大部分情况下复杂度都为$O(1)$，个别情况下复杂度才为$O(n)$
2. 本节的例子中$O(1)$和$O(n)$的出现是非常有规律的，都是一个$O(n)$插入后，紧跟着$n-1$个$O(1)$插入操作
那么如何使用均摊分析呢？在本例中，每一次较为复杂的$O(n)$操作接下来都会跟着$n-1$次的$O(1)$插入操作，那就把复杂操作君叹道接下来的$n-1$次简单操作上，这一组连续的操作的均摊时间复杂度就是$O(1)$

正如前面所说，均摊情况时间复杂度应用场景极为有限，此处简单总结一下：对一个数据结构进行一组连续操作，大部分情况下复杂度很低，个别情况下复杂度较高，切操作之间存在前后连贯的时序关系，此时可以进行均摊分析