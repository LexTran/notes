# 频率派vs贝叶斯派
数学模型
$$
X:data \rightarrow X=(x_1,x_2,...,x_N)^T_{N\times p}\\
\theta:parameter\\
x\thicksim p(x|\theta)
$$
频率派：$\theta$:未知常量，x:r.v(random variable)
MLE准则(最大似然估计)
$$
\theta_{MLE}=\arg \max_{\theta}logP(X|\theta)
$$
频率(优化问题)$\rightarrow$统计机器学习
贝叶斯派：$\theta$:r.v，$\theta\thicksim p(\theta)$称作先验
$$
P(\theta|X)=\frac{P(X|\theta)\cdot P(\theta)}{P(X)}\propto P(X|\theta)\cdot P(\theta)
$$
其中$P(X)=\sum_{\theta}P(X|\theta)P(\theta)$为常量
MAP准则(最大后验估计)
$$
\theta_{MAP}=\arg \max_{\theta}P(\theta|X)=\arg \max_{\theta}P(X|\theta)\cdot P(\theta)
$$
贝叶斯估计
$$
p(\theta|X)=\frac{p(X|theta)\cdot p(\theta)}{\int_{\theta}p(X|\theta)p(\theta)}
$$
贝叶斯(本质上求积分)$\rightarrow$概率图模型

---
# 高斯分布
线性高斯模型举例：Kalman滤波
$$
z_{t}=Az_{t-1}+B+\epsilon
$$
其中，$\epsilon$是高斯噪声

---
$$
data:X=(x_1,x_2,...,x_N)^T=
\begin{bmatrix}
x_1^T\\
x_2^T\\
...\\
x_N^T
\end{bmatrix}_{N\times p}
$$
其中，$x_i\in \mathbb{R}^p$，且$x_i\sim^{iid}N(\mu,\sigma^2)$，$\theta=(\mu,\sigma)$
参数估计，MLE:
$$
\theta_{MLE}=\arg \max_\theta P(X|\theta)
$$
为简化计算，令$p$为一维高斯分布，$\theta=(\mu,\sigma^2)$
$$
\begin{equation}
\begin{aligned}
\log P(X|\theta)&=\log \prod_{i=1}^Np(x_i|\theta)=\sum_{i=1}^N\log p(x_i|\theta)\\
&=\sum_{i=1}^N\log \frac{1}{\sqrt{2\pi}\sigma}\exp[-\frac{(x-\mu)^2}{2\sigma^2}]\\
&=\sum_{i=1}^N[\log\frac{1}{\sqrt{2\pi}}+\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}]\\
\mu_{MLE}&=\arg\max_\mu\log P(X|\theta)\\&=\arg\max_\mu\sum_{i=1}^N[-\frac{(x_i-\mu)^2}{2\sigma^2}]\\&=\arg\max_\mu\sum_{i=1}^N(x_i-\mu)^2\\
&\Rightarrow \frac{\partial}{\partial\mu}\sum_{i=1}^N(x_i-\mu)^2\\
&=\sum_{i=1}^N[-2(x_i-\mu)]\\
&=0
\end{aligned}
\end{equation}
$$
可推得
$$
\sum_{i=1}^N(x_i-\mu)=0
$$
则
$$
\mu_{MLE}=\frac{1}{N}\sum_{i=1}^Nx_i
$$
---
$$
\sigma^2_{MLE}=\arg\max_\sigma P(X|\theta)=\arg\max_\sigma\sum_{i=1}^N [-\log\sigma-\frac{(x_i-\mu)^2}{2\sigma^2}]
$$
记$\sum_{i=1}^N[-\log\sigma -\frac{(x_i-\mu)^2}{2\sigma^2}]$为$\mathcal{L}$
$$
\begin{equation}
\begin{aligned}
\frac{\partial\mathcal{L}}{\partial\sigma}&=\sum_{i=1}^N[-\frac{1}{\sigma}-\frac{1}{2}(x_i-\mu)^2\cdot(-2)\sigma^{-3}]\\
&=\sum_{i=1}^N[-\frac{1}{\sigma}+(x_i-\mu)^2\cdot \sigma^{-3}=0\\
&\Rightarrow \sum_{i=1}^N[-\sigma^2+(x_i-\mu)^2]=0\\
\sigma^2_{MLE}&=\frac{1}{N}\sum_{i=1}^N(x_i-\mu)^2
\end{aligned}
\end{equation}
$$
---
通常，$\sigma^2_{MLE}$是有偏估计，因为$E[\sigma^2_{MLE}]=\frac{N-1}{N}\sigma^2\neq \hat{\sigma^2}=\frac{1}{N}\sum_{i=1}^N(x_i-\mu)^2$，而$E[\mu_{MLE}]=\frac{1}{N}\cdot N\cdot \mu=\mu$，故$\mu_{MLE}$为无偏估计
对于$\sigma$，其无偏估计为$\hat{\sigma}=\frac{1}{N-1}\sum_{i=1}^N(x_i-\mu)^2$

---
回到多维高斯分布
$$
x\sim N(\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
$$
其中，$x\in \mathbb{R}^p$，是一个随机向量(r.v:random vector)，$\mu$也是一个向量，$\Sigma$称为方差矩阵，通常是一个半正定矩阵
对于式$(x-\mu)^T\Sigma^{-1}(x-\mu)$，维度为$(1\times p)\times (p\times p)\times (p\times 1)=1\times 1$，显然最终结果为一个数，实际上这个算式是$x$与$\mu$的马氏距离。当$\Sigma = I$时，马氏距离$=$欧式距离
对方差矩阵进行特征分解，$\Sigma = U \Lambda U^T$，其中$UU^T=U^TU=I$，$\Lambda=diag(\lambda_i)$
故$$\Sigma=\sum_{i=1}^pu_i\lambda_i u_i^T$$则$$
\Sigma^{-1}=(U\Lambda U^T)^{-1}=U\Lambda^{-1}U^T
$$其中$\Lambda^{-1}=diag(\frac{1}{\lambda_i})$，故
$$
\Sigma^{-1}=\sum_{i=1}^pu_i\frac{1}{\lambda_i} u_i^T
$$
原式$(x-\mu)^T\Sigma^{-1}(x-\mu)$可化为$$\sum_{i=1}^p(x-\mu)^Tu_i\frac{1}{\lambda_i} u_i^T(x-\mu)$$
记向量$y$，其中元素$y_i=(x-\mu)^Tu_i$，由维度运算可知$y_i$为一个数，上式可化简为
$$
\sum_{i=1}^py_i\frac{1}{\lambda_i} y_i^T=\sum_{i=1}^p\frac{y_i^2}{\lambda_i}
$$
$y_i$实质上是$(x-\mu)^T$向量在$u_i$方向的投影，将$\sum_{i=1}^p\frac{y_i^2}{\lambda_i}$记为$\Delta$
故多维高斯分布可简写为
$$
\frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}\Delta)
$$
当$p=3$时，根据概率密度可以确定一个二维平面上的椭圆曲线

---
==局限性==
1. $\Sigma_{p\times p}$，由于对称性的存在，参数实际上只有$\frac{p^2-p}{2}+p=\frac{p^2+p}{2}$个，其中$\frac{p^2-p}{2}$是矩阵的上/下三角矩阵减去对角线上参数的个数，$p$是对角线上参数个数
```ad-comment

当$\Sigma$为对角阵时，$y_1$$y_2$方向与$x_1x_2$相同--因子分析(FA)当$\Sigma$为对角阵，且各元素相等时(称作各向同性)，其曲线变成圆--主成分分析(PCA)
```
2. 数据有时无法用一个高斯分布模型描述
```ad-comment
用多个高斯分布描述更加准确--混合高斯模型(GMM)
```
---
引入定理:
$Theorem \quad 1.1$
	已知$x\sim N(\mu, \Sigma)$，有$y=Ax+B$，
	则$y \sim N(A\mu +B, A\Sigma A^T)$
	
一个多维高斯分布(以二维为例)满足如下条件:
	$x=\left( \begin{matrix}x_a \\ x_b\end{matrix}\right)\quad m+n=p \quad \mu=\left( \begin{matrix}\mu_a \\ \mu_b\end{matrix}\right)\quad \Sigma=\left( \begin{matrix}\Sigma_{aa}\quad \Sigma_{ab} \\ \Sigma_{ba}\quad \Sigma_{bb}\end{matrix}\right)$
求它的边缘分布和条件分布

解：
利用$Theorem(1.1)$，可将$x_a$写作$(I_m\quad 0)\left(\begin{matrix}x_a \\ x_b\end{matrix}\right)$，形如$Ax$
故$$x_a\sim N(A\mu_a, A\Sigma A^T)=N(\mu_a, \Sigma_{aa})$$
边缘分布求得

对于条件分布，构造如下随机向量
$$
x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a
$$
则相应的有
$$
\mu_{b\cdot a}=\mu_b -\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\
\Sigma_{bb\cdot a}=\Sigma_{bb} -\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
$$
上述$\Sigma_{bb\cdot a}$称为$\Sigma_{aa}$的Schur补(Schur Complementary)
将$x_{b\cdot a}$拆分为形如$Ax$的矩阵乘法形式，如下
$$
x_{b\cdot a}=(-\Sigma_{ba}\Sigma_{aa}^{-1}\quad I)\left(\begin{matrix}
x_a\\x_b
\end{matrix}\right)
$$
由$Theorem(1.1)$，有
$$
\begin{equation}
\begin{aligned}
E[x_{b\cdot a}]&=(-\Sigma_{ba}\Sigma_{aa}^{-1}\quad I)\left(\begin{matrix}
\mu_a\\ \mu_b
\end{matrix}\right)
\\&=\mu_b -\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a=\mu_{b\cdot a}\\
Var[x_{b\cdot a}]&=
(-\Sigma_{ba}\Sigma_{aa}^{-1}\quad I)\left( \begin{matrix}\Sigma_{aa}\quad \Sigma_{ab} \\ \Sigma_{ba}\quad \Sigma_{bb}\end{matrix}\right)
\left(\begin{matrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T \\ I\end{matrix}\right)\\
&=\Sigma_{bb} -\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
=\Sigma_{bb\cdot a}
\end{aligned}
\end{equation}
$$
故$x_{b\cdot a}\sim N(\mu_{b\cdot a}, \Sigma_{bb\cdot a})$，由$x_{b\cdot a}$的构造可知$x_b=x_{b\cdot a} + \Sigma_{ba}\Sigma_{aa}^{-1}x_a$
又$x_{b\cdot a}$可写为形如$Ax$的形式，故$x_b$可转换为形如$Ax+B$的形式
由$Theorem(1.1)$可知
$$
E[x_b|x_a]=\mu_{b\cdot a} + \Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
Var[x_b|x_a]=Var[x_{b\cdot a}]=\Sigma_{bb\cdot a}
$$
故
$$
x_b|x_a\sim N(\mu_{b\cdot a} + \Sigma_{ba}\Sigma_{aa}^{-1}x_a,\Sigma_{bb\cdot a})
$$

---
一个多维高斯分布满足:
	$p(x)=N(x|\mu, \Lambda^{-1})$，$p(y|x)=N(y|Ax+b, L^{-1})$
其中，$\Lambda^{-1}$称为精度矩阵，它是协方差矩阵的逆矩阵
求$p(y)$和$p(x|y)$

解：
可令$y=Ax+b+\epsilon$，其中$\epsilon \sim N(0,L^{-1})$
则
$$
\begin{equation}
\begin{aligned}
E[y]&=E[Ax+b+\epsilon]\\
&=E[Ax+b]+E[\epsilon]\\&=A\mu+b\\
Var[y]&=Var[Ax+b+\epsilon]\\&=AVar[x]A^T+Var[\epsilon]\\
&=A\Lambda^{-1}A^T+L^{-1}
\end{aligned}
\end{equation}
$$
故$$y \sim N(A\mu+b,A\Lambda^{-1}A^T+L^{-1})$$
$p(y)$求得
对于$p(x|y)$，构造随机向量$z=\left(\begin{matrix}x\\y\end{matrix}\right)$
显然，有$$z \sim N(\left[\begin{matrix}\mu\\A\mu+b\end{matrix}\right],\left[\begin{matrix}\Lambda^{-1}\qquad \qquad \quad \Delta\\\Delta\quad L^{-1}+A\Lambda^{-1}A^T\end{matrix}\right])$$
其中$\Delta$实质上是$x$和$y$的协方差，即$$\Delta=E[(x-E[x])(y-E[y])^T]=E[(x-\mu)(y-A\mu-b)^T]$$
将$y=Ax+b+\epsilon$代入
$$
\begin{equation}
\begin{aligned}
\Delta&=E[(x-\mu)(Ax-A\mu+\epsilon)^T]\\
&=E[(x-\mu)(Ax-A\mu)^T+(x-\mu)\epsilon^T]\\
&=E[(x-\mu)(Ax-A\mu)^T]=E[(x-\mu)(x-\mu)^TA^T]\\
&=Var[x]A^T=\Lambda^{-1}A^T
\end{aligned}
\end{equation}
$$

则$p(x|y)=N(\mu_{x\cdot y},\Sigma_{xx\cdot y})$

---
# 线性回归

## 最小二乘法

**矩阵表达**

$设数据集D=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，$x_i\in \mathbb R^p，y_i\in\mathbb R\quad i=1,2,...,N$

令
$$
X=(x_1,x_2,...,x_N)^T=\left(\begin{matrix}x_{11}\quad x_{12}\quad...\quad x_{1p}\\x_{21}\quad x_{22}\quad...\quad x_{2p}\\...\quad...\quad...\\x_{N1}\quad x_{N2}\quad...\quad x_{Np}\end{matrix}\right)\\
Y=\left(\begin{matrix}y_1\\y_2\\...\\y_N\end{matrix}\right)
$$
数据拟合的函数直线我们设为$f(w)=w^Tx+b$，将其中$b$视作$w_0x_0$，其中$x_0=1$故可简化为$f(w)=w^Tx$

最小二乘估计：

定义如下误差函数
$$
\begin{equation}
\begin{aligned}
L(w)&=\sum_{i=1}^N\lvert\lvert w^Tx_i-y_i\rvert\rvert^2\\
&=\sum_{i=1}^N(w^Tx_i-y_i)^2\\
&=(w^Tx_1-y_1\quad w^Tx_2-y_2\quad...\quad w^Tx_N-y_N)\left(\begin{matrix}w^Tx_1-y_1\\w^Tx_2-y_2\\...\\w^Tx_N-y_N\end{matrix}\right)\\
&=(w^TX^T-Y^T)(w^TX^T-Y^T)^T\\
&=(w^TX^T-Y^T)(Xw-Y)\\
&=w^TX^TXw-w^TX^TY-Y^TXw+Y^TY\\
&=w^TX^TXw-2w^TX^TY+Y^TY
\end{aligned}
\end{equation}
$$
则
$$
\begin{equation}
\begin{aligned}
\hat{w}&=\arg \min L(w)\\
\frac{\partial L(w)}{\partial w}&=2X^TXw-2X^TY=0\\
\therefore X^TXw&=X^TY\\
\Rightarrow w&=(X^TX)^{-1}X^TY
\end{aligned}
\end{equation}
$$
==注上述推导涉及矩阵的求导法则==

其中$(X^TX)^{-1}X^T$称作伪逆，记作$X^{\dagger}$

---
**几何意义**

换个角度思考，拟合函数设为$f(\beta)=x^T\beta$，此时的$p$个$N$维列向量$x_i$构成$p$维子空间，实际的数据值$y$必定不在此子空间上，事实上我们所想要的就是此子空间上与$y$距离最近的向量，该向量就是拟合函数，也即$X\beta$，则误差函数就是子空间上拟合函数向量与$y$的距离，即
$$
L(\beta)=(Y-X\beta)
$$
显然，$(Y-X\beta)$垂直于子空间平面，故$X^T(Y-X\beta)=\mathtt 0$

可以反解出
$$
X^TY=X^TX\beta\\
\Rightarrow \beta=(X^TX)^{-1}X^TY
$$
---

## 最小二乘法等价于噪声为高斯分布的极大似然估计(概率角度)

---

## 正则化

### $L_1 \rightarrow$ Lasso

### $L_2\rightarrow$ 岭回归